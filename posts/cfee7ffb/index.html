<!DOCTYPE html>
<html lang="zh-Hans">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.4.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/favicon.jpeg">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon.jpeg">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon.jpeg">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"hustclf.github.io","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="kafka streams 是什么Kafka 号称是一个开源的分布式流数据处理平台，其除了提供基本的 Consumer api 和 Producer api 用于处理基本的消费和生成数据外，还抽象和封装了功能更强大的 Streams api 用于实现基于 kafka 的流式计算。不同于flink、spark 等其他框架，kafka streams 仅仅是一个 java library，但通过深度结">
<meta property="og:type" content="article">
<meta property="og:title" content="kafka streams：一款轻量级流式计算引擎">
<meta property="og:url" content="https://hustclf.github.io/posts/cfee7ffb/index.html">
<meta property="og:site_name" content="大飞哥的博客">
<meta property="og:description" content="kafka streams 是什么Kafka 号称是一个开源的分布式流数据处理平台，其除了提供基本的 Consumer api 和 Producer api 用于处理基本的消费和生成数据外，还抽象和封装了功能更强大的 Streams api 用于实现基于 kafka 的流式计算。不同于flink、spark 等其他框架，kafka streams 仅仅是一个 java library，但通过深度结">
<meta property="og:locale">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/1.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/2.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/3.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/4.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/5.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/6.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/7.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/8.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/9.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/10.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/11.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/12.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/13.png">
<meta property="og:image" content="https://hustclf.github.io/images/kafka_streams/14.png">
<meta property="article:published_time" content="2022-02-20T12:34:08.000Z">
<meta property="article:modified_time" content="2022-07-15T12:03:09.435Z">
<meta property="article:author" content="大飞哥">
<meta property="article:tag" content="kafka streams">
<meta property="article:tag" content="流计算">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://hustclf.github.io/images/kafka_streams/1.png">

<link rel="canonical" href="https://hustclf.github.io/posts/cfee7ffb/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-Hans'
  };
</script>

  <title>kafka streams：一款轻量级流式计算引擎 | 大飞哥的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="大飞哥的博客" type="application/atom+xml">
</head>

<body itemscope="" itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope="" itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">大飞哥的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">借书满架，偃仰啸歌，冥然兀坐，万籁有声</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-首页">

    <a href="/" rel="section"><i class="home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-关于">

    <a href="/about/" rel="section"><i class="user fa-fw"></i>关于</a>

  </li>
        <li class="menu-item menu-item-开源">

    <a href="/contributions/" rel="section"><i class="github fa-fw"></i>开源</a>

  </li>
        <li class="menu-item menu-item-标签">

    <a href="/tags/" rel="section"><i class="tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-分类">

    <a href="/categories/" rel="section"><i class="th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-归档">

    <a href="/archives/" rel="section"><i class="archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope="" itemtype="http://schema.org/Article" class="post-block" lang="zh-Hans">
    <link itemprop="mainEntityOfPage" href="https://hustclf.github.io/posts/cfee7ffb/">

    <span hidden itemprop="author" itemscope="" itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="大飞哥">
      <meta itemprop="description" content="大数据、云原生、微服务等">
    </span>

    <span hidden itemprop="publisher" itemscope="" itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="大飞哥的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          kafka streams：一款轻量级流式计算引擎
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-02-20 20:34:08" itemprop="dateCreated datePublished" datetime="2022-02-20T20:34:08+08:00">2022-02-20</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-07-15 20:03:09" itemprop="dateModified" datetime="2022-07-15T20:03:09+08:00">2022-07-15</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope="" itemtype="http://schema.org/Thing">
                  <a href="/categories/%E5%A4%A7%E6%95%B0%E6%8D%AE/" itemprop="url" rel="index"><span itemprop="name">大数据</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h2 id="kafka-streams-是什么"><a href="#kafka-streams-是什么" class="headerlink" title="kafka streams 是什么"></a>kafka streams 是什么</h2><p>Kafka 号称是一个开源的分布式流数据处理平台，其除了提供基本的 Consumer api 和 Producer api 用于处理基本的消费和生成数据外，还抽象和封装了功能更强大的 Streams api 用于实现基于 kafka 的流式计算。不同于flink、spark 等其他框架，kafka streams 仅仅是一个 java library，但通过深度结合 kafka 的种种高级特性，实现了一个轻量级、功能完备的流式计算框架， kafka streams 承载着 kafka 在流计算领域大展拳脚的野心，也逐渐成为 kafka 项目越发重要的组件。</p>
<h3 id="流计算简介"><a href="#流计算简介" class="headerlink" title="流计算简介"></a>流计算简介</h3><p>借用 flink 官网的一张批量计算和流计算的图，我们能对二者的用户场景有一个较清晰的认识。<br><img src="/images/kafka_streams/1.png"></p>
<p>批量计算<br><img src="/images/kafka_streams/2.png"></p>
<p>流式计算<br><img src="/images/kafka_streams/3.png"></p>
<span id="more"></span>

<img src="/images/kafka_streams/4.png">


<p>典型的批量计算场景如获取去年的用户地域分布情况，典型的流计算场景如阿里双11的实时成交额。</p>
<h3 id="kafka-streams"><a href="#kafka-streams" class="headerlink" title="kafka streams"></a>kafka streams</h3><p>kafka streams 是 kafka 提供的一个 api library， 类似于Producer api 和 Consumer api, kafka streams 实时消费上游的 topic，经过自定义的计算，将结果生产到下游的 topic，理论上你可以自己写调用 Producer api 和 Consumer api来实现一个流计算应用，那与之相比， kafka streams 的优势在哪呢？</p>
<ol>
<li>流计算不仅仅是 consume + produce: 流计算框架更贴近业务，其抽象了诸多概念如时间定义（生成时间、处理时间、事件时间）、时间窗口（sliding window、hopping window、event window等）、处理晚到数据、map-reduce、table join 等标准功能，使用底层api 实现会费时费力，且不可复用。</li>
<li>流计算需要状态和执行语义保证： 大部分的流计算任务都是有状态的，部分计算任务也是有exactly-once 等语义保证的。使用底层api 比较难统一的抽象存储和实现保证的语义。</li>
</ol>
<p>因此可以这样理解 kafka streams，kakfa streams 是一个用于实现流计算任务的 java library， 它底层使用了Producer api 和 Consumer api，并封装了 Time、TimeWindows、StateStore、Kstreams Ktable、Topology 等高级对象对流计算任务进行了完整的抽象，也完整地支持了 exactly-once等语义。</p>
<h3 id="kafka-streams-同其他流计算框架对比"><a href="#kafka-streams-同其他流计算框架对比" class="headerlink" title="kafka streams 同其他流计算框架对比"></a>kafka streams 同其他流计算框架对比</h3><p>相比storm、spark streaming、flink 这些流计算框架，kafka streams 有何特点呢？</p>
<ul>
<li>轻量。Kafka Streams 仅仅是一个 java Library，它可以非常方便地嵌入任意Java应用中，也可以任意方式打包和部署</li>
<li>依赖少。 运行kafka stream 实现的流计算任务，仅需要 kafka</li>
<li>完备的流计算语义。如支持EventTimeWindow，支持处理晚到的数据，支持 Stream 和 Table 的抽象、支持 exactly-once等</li>
<li>充分利用Kafka分区机制实现水平扩展和顺序性保证</li>
</ul>
<h2 id="kakfa-streams-示例：-WordCount"><a href="#kakfa-streams-示例：-WordCount" class="headerlink" title="kakfa streams 示例： WordCount"></a>kakfa streams 示例： WordCount</h2><p>我们以一个kafka streams 的入门示例 WordCount 来进行后续 kafka streams 原理的深入介绍，此处只简单描述下该示例的功能。 这个实例实现了单测计数的功能，用户往 streams-plaintext-input 里持续写入一些句子，运行该应用后，会在WordsWithCountsTopic 中持续输出单词计数的变化。</p>
<figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// example code from https://kafka.apache.org/documentation/streams/</span></span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.serialization.Serdes;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.common.utils.Bytes;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.KafkaStreams;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsBuilder;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.StreamsConfig;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KStream;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.KTable;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.Materialized;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.kstream.Produced;</span><br><span class="line"><span class="keyword">import</span> org.apache.kafka.streams.state.KeyValueStore;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.util.Arrays;</span><br><span class="line"><span class="keyword">import</span> java.util.Properties;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">WordCountApplication</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(<span class="keyword">final</span> String[] args)</span> <span class="keyword">throws</span> Exception </span>&#123;</span><br><span class="line">        Properties props = <span class="keyword">new</span> Properties();</span><br><span class="line">        props.put(StreamsConfig.APPLICATION_ID_CONFIG, <span class="string">&quot;wordcount-application&quot;</span>);</span><br><span class="line">        props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, <span class="string">&quot;localhost:9092&quot;</span>);</span><br><span class="line">        props.put(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line">        props.put(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.String().getClass());</span><br><span class="line"></span><br><span class="line">        StreamsBuilder builder = <span class="keyword">new</span> StreamsBuilder();</span><br><span class="line">        KStream&lt;String, String&gt; textLines = builder.stream(<span class="string">&quot;streams-plaintext-input&quot;</span>);</span><br><span class="line">        KTable&lt;String, Long&gt; wordCounts = textLines</span><br><span class="line">            .flatMapValues(textLine -&gt; Arrays.asList(textLine.toLowerCase().split(<span class="string">&quot;\\W+&quot;</span>)))</span><br><span class="line">            .groupBy((key, word) -&gt; word)</span><br><span class="line">            .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, <span class="keyword">byte</span>[]&gt;&gt;as(<span class="string">&quot;counts-store&quot;</span>));</span><br><span class="line">        wordCounts.toStream().to(<span class="string">&quot;streams-wordcount-output&quot;</span>, Produced.with(Serdes.String(), Serdes.Long()));</span><br><span class="line"></span><br><span class="line">        KafkaStreams streams = <span class="keyword">new</span> KafkaStreams(builder.build(), props);</span><br><span class="line">        streams.start();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>


<p>运行的效果如下所示：</p>
<p>向 streams-plaintext-input 写入数据</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic streams-plaintext-input</span><br><span class="line">all streams lead to kafka</span><br><span class="line">hello kafka streams</span><br><span class="line">join kafka summit</span><br></pre></td></tr></table></figure>

<p>从streams-wordcount-output 消费结果</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">&gt; bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 \</span><br><span class="line">--topic streams-wordcount-output \</span><br><span class="line">--from-beginning \</span><br><span class="line">--formatter kafka.tools.DefaultMessageFormatter \</span><br><span class="line">--property print.key=true \</span><br><span class="line">--property print.value=true \</span><br><span class="line">--property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer \</span><br><span class="line">--property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer</span><br><span class="line"></span><br><span class="line">all	    1</span><br><span class="line">streams	1</span><br><span class="line">lead	1</span><br><span class="line">to	    1</span><br><span class="line">kafka	1</span><br><span class="line">hello	1</span><br><span class="line">kafka	2</span><br><span class="line">streams	2</span><br><span class="line">join	1</span><br><span class="line">kafka	3</span><br><span class="line">summit	1</span><br></pre></td></tr></table></figure>

<p>完整的实例请参考：<a target="_blank" rel="external nofollow noopener noreferrer" href="https://kafka.apache.org/31/documentation/streams/quickstart">https://kafka.apache.org/31/documentation/streams/quickstart)</a></p>
<p>这个例子实现了一个可横向扩展的、错误容忍、有状态的流计算任务，后续内容会基于此例子进行深入的介绍。</p>
<h2 id="kafka-streams-的架构"><a href="#kafka-streams-的架构" class="headerlink" title="kafka streams 的架构"></a>kafka streams 的架构</h2><img src="/images/kafka_streams/5.png">
kafka streams 运行的一个示例如上图所示，最上方是streams-plaintext-input，有 4 个 partition，最下面是streams-wordcount-output， 也是 4 个 partition，中间可以理解为单台机器上的一个 java 程序，运行了 WordCount，设置以2个 thread来运行。 定义kafka 地址，input topic 和 output topic 的代码如下：

<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">// 定义 kafka streams 的 appid, 会基于其生成 consumer group id</span><br><span class="line">props.put(StreamsConfig.APPLICATION_ID_CONFIG, &quot;wordcount-application&quot;);</span><br><span class="line">// kafka broker 地址</span><br><span class="line">props.put(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;localhost:9092&quot;);</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// 读取 kafka streams 的 input topic</span><br><span class="line">KStream&lt;String, String&gt; textLines = builder.stream(&quot;streams-plaintext-input&quot;);</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">// 将输出结果写入 output topic</span><br><span class="line">wordCounts.toStream().to(&quot;streams-wordcount-output&quot;, Produced.with(Serdes.String(),</span><br></pre></td></tr></table></figure>

<p>每个 thread 里有以下内容：</p>
<ul>
<li>一个 consumer 和 producer，用于消费上游数据和将计算结果写入下游数据。</li>
<li>各有2个 task，每个 task 拥有相同且彼此独立的有向无环图，有向无环图图描述了流计算的具体逻辑，每个 task 有独立的 state store 用来存储计算产生的状态数据。</li>
<li>有 4 个 task 是因为上游有4 个 partition，每个 task 负责处理一个 partition的数据。</li>
</ul>
<h3 id="并发模型"><a href="#并发模型" class="headerlink" title="并发模型"></a>并发模型</h3><p>上述实例展示了单实例多线程的运行情况。一般分布式计算任务较大时很难通过单个任务完成，kafka streams 应用是如何实现自动扩缩容的呢？</p>
<p>kafka streams 充分利用了 kafka 的 consumer group 机制，能动态感知 consumer 的变化，针对节点数、kafka streams 实例数、线程数的变化进行tasks 的分配，以实现无缝的扩缩容。</p>
<h4 id="缩容"><a href="#缩容" class="headerlink" title="缩容"></a>缩容</h4><p>单节点单实例单线程： 由唯一的线程处理 4 个 tasks。<br><img src="/images/kafka_streams/6.png"></p>
<h4 id="扩容"><a href="#扩容" class="headerlink" title="扩容"></a>扩容</h4><p>单节点单实例多线程: 4 个 tasks 平均地分配给 2 个 thread。<br><img src="/images/kafka_streams/7.png"></p>
<p>单节点多实例多线程： 4 个 tasks 分配给 2 个 kafka streams 实例的 3 个 thread<br><img src="/images/kafka_streams/8.png"></p>
<p>多节点多实例多线程: 4 个 task分配给 4 个 kafka streams 实例，这4个实例分布在 2 个 节点上。<br><img src="/images/kafka_streams/9.png"></p>
<h4 id="动态分配-tasks"><a href="#动态分配-tasks" class="headerlink" title="动态分配 tasks"></a>动态分配 tasks</h4><p>kafka streams 还支持动态的 tasks 分配，假如从已运行的多个 kafka streams 实例中删除一个实例，其他的 kafka streams 实例会接管其分配的 tasks，重建 state store，并继续执行。 正是这种无缝动态扩缩容的机制赋予了kafka streams 灵活的部署方式，能根据实际的计算资源进行动态的适配。而且能够以部署无状态服务的方式部署 kafka streams 应用。</p>
<h4 id="限制"><a href="#限制" class="headerlink" title="限制"></a>限制</h4><p>最大扩容数：</p>
<p>由于上游topic 的 partition 数量和 tasks 数量是一一对应的，因此最大分配的 tasks 数量是固定的，当kafka streams 实例数超过 tasks 数量时，多余的kafka streams 实例会由于得不到分配的 tasks 任务而处于 pending 状态。</p>
<p>tasks 重新分配：</p>
<p>tasks 重新分配需要 state store 重建然后才能继续执行 task， 重建 state store 会优先从本地磁盘恢复，若找不到本地磁盘信息则会通过远端 kafka 集群的备份 topic 来重建这些信息，可能会耗时较长。</p>
<h3 id="有向无环图（Topology）"><a href="#有向无环图（Topology）" class="headerlink" title="有向无环图（Topology）"></a>有向无环图（Topology）</h3><p>kafka streams 的 task 的是以有向无环图图(Topology) 的方式进行抽象的，如下图所示<br><img src="/images/kafka_streams/10.png"></p>
<p>topology 中 processor 有两种比较特殊</p>
<p>source processor: 定义了 topology 的入口，即从哪些topic 读取数据<br>sink processor: 定义了 topology 的出口，即将最终结果写入哪些 topic<br>在上述 WorkCount 示例的代码，以 DSL 的方式生成最终执行的有向无环图</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">// 初始化一个builder，用于最终生成 topology</span><br><span class="line">StreamsBuilder builder = new StreamsBuilder();</span><br><span class="line">// 在 topology 中创建 source processor</span><br><span class="line">KStream&lt;String, String&gt; textLines = builder.stream(&quot;streams-plaintext-input&quot;);</span><br><span class="line">// 描述 topology，会生成各种中间的 steam processor</span><br><span class="line">KTable&lt;String, Long&gt; wordCounts = textLines</span><br><span class="line">    .flatMapValues(textLine -&gt; Arrays.asList(textLine.toLowerCase().split(&quot;\\W+&quot;)))</span><br><span class="line">    .groupBy((key, word) -&gt; word)</span><br><span class="line">    .count(Materialized.&lt;String, Long, KeyValueStore&lt;Bytes, byte[]&gt;&gt;as(&quot;counts-store&quot;));</span><br><span class="line">//在 topology 中创建 sink processor</span><br><span class="line"> wordCounts.toStream().to(&quot;streams-wordcount-output&quot;, Produced.with(Serdes.String(), Serdes.Long())); KafkaStreams streams = new KafkaStreams(builder.build(), props); streams.start();</span><br></pre></td></tr></table></figure>


<p>打印有向无环图（topology）</p>
<p>kafka streams 支持打印生成的 topology，便于用户能更深入地理解 topology 的细节，示例代码</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">// 通过 StreamBuilder 生成 有向无环图</span><br><span class="line">final Topology topology = builder.build();</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">// 打印有向无环图</span><br><span class="line">System.out.println(topology.describe());</span><br></pre></td></tr></table></figure>

<p>一个最简单的 topology （非WordCount）打印结果如下, 从结果可看出 topology 只有两个 processor。</p>
<ul>
<li>KSTREAM-SOURCE-0000000000： 一个 source processor，从streams-plaintext-input 读取数据</li>
<li>KSTREAM-SINK-0000000001： 一个 sink processor，从KSTREAM-SOURCE-0000000000 拿到数据，并写入streams-pipe-output。</li>
</ul>
<p>因此这个拓扑图代表的计算任务仅仅是从streams-plaintext-input 读取数据，再写到streams-pipe-output 中，没有进行任何的计算。</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt; mvn clean package</span><br><span class="line">&gt; mvn exec:java -Dexec.mainClass=myapps.Pipe</span><br><span class="line">Sub-topologies:</span><br><span class="line">Sub-topology: 0</span><br><span class="line">Source: KSTREAM-SOURCE-0000000000(topics: streams-plaintext-input) --&gt; KSTREAM-SINK-0000000001</span><br><span class="line">Sink: KSTREAM-SINK-0000000001(topic: streams-pipe-output) &lt;-- KSTREAM-SOURCE-0000000000</span><br><span class="line">Global Stores:</span><br><span class="line">none</span><br></pre></td></tr></table></figure>


<h3 id="状态存储"><a href="#状态存储" class="headerlink" title="状态存储"></a>状态存储</h3><p>在流计算任务中不可避免会使用到状态存储。 如WordCount 的示例中，需要在计算时保存每个单词出现的计数。<br><img src="/images/kafka_streams/11.png"></p>
<p>Kafka Stream 可以为每个流任务嵌入一个或多个本地状态存储，并且允许开发者通过API的方式进行访问、查询所需要处理的数据。这些状态数据底层默认是基于 RocksDB 数据库实现的，本质其实是在内存的一个 hashmap。 而且状态存储默认支持同步到远端的 kafka broker，以 topic 的方式记录本地存储的 changelog。 当kafka streams 的task 因重建或者动态扩缩容而发生迁移时，若迁移后的kafka streams 实例无法从本地磁盘恢复状态，就会读取下changelog 的topic来重建状态存储。</p>
<p>更多细节可以参考：<a target="_blank" rel="external nofollow noopener noreferrer" href="https://kafka.apache.org/31/documentation/streams/architecture#streams_architecture_recovery">https://kafka.apache.org/31/documentation/streams/architecture#streams_architecture_recovery</a></p>
<h2 id="kafka-streams-与流计算"><a href="#kafka-streams-与流计算" class="headerlink" title="kafka streams 与流计算"></a>kafka streams 与流计算</h2><p>类似其他流计算框架，kafka streams 支持通用的流计算语义。如时间、时间窗口、流和表的转换、exectly-once 的执行保证等</p>
<h3 id="时间"><a href="#时间" class="headerlink" title="时间"></a>时间</h3><p>kakfa streams 分为 3种时间定义：</p>
<ul>
<li>Event time： 事件时间。 这也是大多数场景使用的时间。我们希望写入topic 的日志能表述其真正发生的时间，后续做相关计算时需要基于该时间。比如要统计一个用户每天登陆 qq 的次数。 在写入kafka 时可以指定 timestamp 来表述该时间</li>
<li>Processing time：日志在kafka streams 任务中处理时的时间。</li>
<li>Ingestion time： 日志被写进 kafka 的 topic 的时间。</li>
</ul>
<h3 id="窗口"><a href="#窗口" class="headerlink" title="窗口"></a>窗口</h3><p>kafka streams 的 DSL 也提供了多种时间窗口</p>
<p>窗口类型    表现    描述<br>Hopping time window    基于时间    大小固定、重叠的窗口<br>Tumbling time window    基于时间<br>大小固定、不重叠的窗口<br>Sliding time window    基于时间<br>大小固定，重叠的窗口，仅用于join 计算的窗口<br>Session window    基于session<br>大小不固定、不重叠的、基于数据驱动的窗口<br>Hopping time window</p>
<p>定义一个大小为5分钟、间隔为1 分钟的Hopping time window。</p>
<p>代码定义</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">// A hopping time window with a size of 5 minutes and an advance interval of 1 minute.</span><br><span class="line">// The window&#x27;s name -- the string parameter -- is used to e.g. name the backing state store.</span><br><span class="line">Duration windowSize = Duration.ofMinutes(5);</span><br><span class="line">Duration advance = Duration.ofMinutes(1);</span><br><span class="line">TimeWindows.ofSizeWithNoGrace(windowSize).advanceBy(advance);</span><br></pre></td></tr></table></figure>


<img src="/images/kafka_streams/12.png">


<p>Tumbling time window</p>
<p>定义一个大小为5分钟的Tumbling time window。</p>
<p>代码定义</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">// A tumbling time window with a size of 5 minutes (and, by definition, an implicit</span><br><span class="line">// advance interval of 5 minutes), and grace period of 1 minute.</span><br><span class="line">Duration windowSize = Duration.ofMinutes(5);</span><br><span class="line">Duration gracePeriod = Duration.ofMinutes(1);</span><br><span class="line">TimeWindows.ofSizeAndGrace(windowSize, gracePeriod);</span><br><span class="line"></span><br><span class="line">// The above is equivalent to the following code:</span><br><span class="line">TimeWindows.ofSizeAndGrace(windowSize, gracePeriod).advanceBy(windowSize);</span><br></pre></td></tr></table></figure>

<p>Sliding time windows 主要用于 join 操作，具体可参考：<a target="_blank" rel="external nofollow noopener noreferrer" href="https://kafka.apache.org/31/documentation/streams/developer-guide/dsl-api#sliding-time-windows">https://kafka.apache.org/31/documentation/streams/developer-guide/dsl-api#sliding-time-windows</a><br><img src="/images/kafka_streams/13.png"></p>
<p>Session Windows： session windows 是窗口固定的、但以事件驱动开始时间的窗口。</p>
<p>定义一个长度为 5 分钟的Session window</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">import java.time.Duration;</span><br><span class="line">import org.apache.kafka.streams.kstream.SessionWindows;</span><br><span class="line"></span><br><span class="line">// A session window with an inactivity gap of 5 minutes.</span><br><span class="line">SessionWindows.ofInactivityGapWithNoGrace(Duration.ofMinutes(5));</span><br></pre></td></tr></table></figure>


<img src="/images/kafka_streams/14.png">

<p>相同的颜色代表相同的kafka 的key，session windows 的开始时间是key 第一次出现的时间。 一个典型的 session window 的使用场景是统计一个网站每个用户的活跃次数。 用户登录及5分钟内的操作认为是一次活跃，则每个 session windows 的开始时间是一次登录日志，这之后五分钟内的任何操作都会认为属于同一次活跃行为。</p>
<h3 id="执行保证"><a href="#执行保证" class="headerlink" title="执行保证"></a>执行保证</h3><p>kafka streams 支持 exactly-once，这也是理想的流计算引擎需要支持的功能，这可以保证一条数据写入kafka后，所有的计算都只发生一次。 要实现这样的语义并不容易，因为 kafka streams 的典型场景是 consume + compute + produce， 失败随时都会发生，如从上游消费失败、计算失败、写入下游失败，都会阻碍 exactly-once 语义的实现。</p>
<p>kakfa streams 通过使用 kafka 的一些高级特性实现了在kafka 系统内端到端的exactly-once。 主要使用了如下特性：</p>
<p>producer 写入单个partition的幂等性：即在同一个partition 写入多条相同的数据，只会生效一次<br>producer 写入多partition的原子性： producer batch 写入多个partition时，要么同时成功，要么同时失败。<br>consumer 设置事务隔离级为read_commited，即只消费上游已提交事务的信息。<br>这里的实现逻辑比较复杂，推荐以下文章，感兴趣的同学可以深入看下：</p>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.confluent.io/blog/enabling-exactly-once-kafka-streams/">https://www.confluent.io/blog/enabling-exactly-once-kafka-streams/</a></p>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/">https://www.confluent.io/blog/exactly-once-semantics-are-possible-heres-how-apache-kafka-does-it/</a></p>
<p><a target="_blank" rel="external nofollow noopener noreferrer" href="https://www.confluent.io/blog/simplified-robust-exactly-one-semantics-in-kafka-2-5/">https://www.confluent.io/blog/simplified-robust-exactly-one-semantics-in-kafka-2-5/</a></p>
<h2 id="kafka-streams-编程模型"><a href="#kafka-streams-编程模型" class="headerlink" title="kafka streams 编程模型"></a>kafka streams 编程模型</h2><p>kafka streams 的核心是生成有向无环图（Topology）来描述计算任务，它提供了两种编程模型来实现该操作</p>
<h3 id="Processor-API"><a href="#Processor-API" class="headerlink" title="Processor API"></a>Processor API</h3><p>低级别的 api， 需要开发者来描述整个拓扑图的算子，以及定义及使用 state store，上手难度大，灵活性强。</p>
<h3 id="DSL"><a href="#DSL" class="headerlink" title="DSL"></a>DSL</h3><p>Domain Specific Language，是定义在 Processor API 之上更高级的编程模型，它屏蔽了很多底层实现细节，提供了 Window、Join等高级功能，可以通过 KStream 和 KTable 的各种 map reduce join 的操作来生成执行的 Topology。 本文最上面提供的示例就是使用 DSL 来实现的。当然也可以使用 Process API 来实现，但需要开发者考虑的部分就较多了。</p>
<p>DSL 能满足绝大部分的计算场景。</p>
<h3 id="DSL-和-Processor-API-的结合"><a href="#DSL-和-Processor-API-的结合" class="headerlink" title="DSL 和 Processor API 的结合"></a>DSL 和 Processor API 的结合</h3><p>某些复杂的业务场景可能 DSL 无法满足、或者实现生成的 Topology 比较冗余，也可以结合DSL 和 Processor API 进行更丰富的描述。如下是一个示例，该例子实现了当网页访问的次数超过1000次时发送邮件提醒管理员。Processor 相比仅仅在DSL 中 iterate 每条记录能实现更高级的功能，比如为了避免发送邮件的次数过多，可以在 Processor 里定义一个 state store，仅仅在网页第一次访问超过1000次时进行提醒，之后不再提醒避免邮件告警淹没。 由于 Processor 强大的可扩展性可以很方便地实现上述功能，而这在单纯的DSL里就比较难定义、或者实现的性能较差。</p>
<p>processor 的定义</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">// A processor that sends an alert message about a popular page to a configurable email address</span><br><span class="line">public class PopularPageEmailAlert implements Processor &lt; PageId, Long, Void, Void &gt; &#123;</span><br><span class="line"></span><br><span class="line">    private final String emailAddress;</span><br><span class="line">    private ProcessorContext &lt; Void,</span><br><span class="line">    Void &gt; context;</span><br><span class="line"></span><br><span class="line">    public PopularPageEmailAlert(String emailAddress) &#123;</span><br><span class="line">        this.emailAddress = emailAddress;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void init(ProcessorContext &lt; Void, Void &gt; context) &#123;</span><br><span class="line">        this.context = context;</span><br><span class="line"></span><br><span class="line">        // Here you would perform any additional initializations such as setting up an email client.</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    void process(Record &lt; PageId, Long &gt; record) &#123;</span><br><span class="line">        // Here you would format and send the alert email.</span><br><span class="line">        //</span><br><span class="line">        // In this specific example, you would be able to include</span><br><span class="line">        // information about the page&#x27;s ID and its view count</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    void close() &#123;</span><br><span class="line">        // Any code for clean up would go here, for example tearing down the email client and anything</span><br><span class="line">        // else you created in the init() method</span><br><span class="line">        // This processor instance will not be used again after this call.</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>DSL 中使用 processor</p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">KStream &lt; String, GenericRecord &gt; pageViews = ...;</span><br><span class="line"></span><br><span class="line">// Send an email notification when the view count of a page reaches one thousand.</span><br><span class="line">pageViews.groupByKey()</span><br><span class="line">    .count()</span><br><span class="line">    .filter((PageId pageId, Long viewCount) - &gt; viewCount == 1000)</span><br><span class="line">    // PopularPageEmailAlert is your custom processor that implements the</span><br><span class="line">    // `Processor` interface, see further down below.</span><br><span class="line">    .process(() - &gt; new PopularPageEmailAlert(&quot;alerts@yourcompany.com&quot;));</span><br></pre></td></tr></table></figure>

<p>测试编写<br>kafka streams 也提供了 测试相关的库，它不需要开发者起真正的kafka 集群就能进行基本的业务逻辑的测试，主要通过 mock了有向无环图的 source processor 和 sink processor，抽象了输入和输出的定义。</p>
<p>以下是一个Processor API 开发的应用的测试 case</p>
<p>setup 定义了 Topology的描述<br>tearDown 定义了测试的销毁逻辑<br>@Test 注解定义了具体的测试用例<br>参考：<a target="_blank" rel="external nofollow noopener noreferrer" href="https://kafka.apache.org/31/documentation/streams/developer-guide/testing.html">https://kafka.apache.org/31/documentation/streams/developer-guide/testing.html</a></p>
<figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br></pre></td><td class="code"><pre><span class="line">private TopologyTestDriver testDriver;</span><br><span class="line">private TestInputTopic &lt; String, Long &gt; inputTopic;</span><br><span class="line">private TestOutputTopic &lt; String, Long &gt; outputTopic;</span><br><span class="line">private KeyValueStore &lt; String, Long &gt; store;</span><br><span class="line"></span><br><span class="line">private Serde &lt; String &gt; stringSerde = new Serdes.StringSerde();</span><br><span class="line">private Serde &lt; Long &gt; longSerde = new Serdes.LongSerde();</span><br><span class="line"></span><br><span class="line">@Before</span><br><span class="line">public void setup() &#123;</span><br><span class="line">    final Topology topology = new Topology();</span><br><span class="line">    topology.addSource(&quot;sourceProcessor&quot;, &quot;input-topic&quot;);</span><br><span class="line">    topology.addProcessor(&quot;aggregator&quot;, new CustomMaxAggregatorSupplier(), &quot;sourceProcessor&quot;);</span><br><span class="line">    topology.addStateStore(</span><br><span class="line">        Stores.keyValueStoreBuilder(</span><br><span class="line">            Stores.inMemoryKeyValueStore(&quot;aggStore&quot;),</span><br><span class="line">            Serdes.String(),</span><br><span class="line">            Serdes.Long()).withLoggingDisabled(), // need to disable logging to allow store pre-populating</span><br><span class="line">        &quot;aggregator&quot;);</span><br><span class="line">    topology.addSink(&quot;sinkProcessor&quot;, &quot;result-topic&quot;, &quot;aggregator&quot;);</span><br><span class="line"></span><br><span class="line">    // setup test driver</span><br><span class="line">    final Properties props = new Properties();</span><br><span class="line">    props.setProperty(StreamsConfig.APPLICATION_ID_CONFIG, &quot;maxAggregation&quot;);</span><br><span class="line">    props.setProperty(StreamsConfig.BOOTSTRAP_SERVERS_CONFIG, &quot;dummy:1234&quot;);</span><br><span class="line">    props.setProperty(StreamsConfig.DEFAULT_KEY_SERDE_CLASS_CONFIG, Serdes.String().getClass().getName());</span><br><span class="line">    props.setProperty(StreamsConfig.DEFAULT_VALUE_SERDE_CLASS_CONFIG, Serdes.Long().getClass().getName());</span><br><span class="line">    testDriver = new TopologyTestDriver(topology, props);</span><br><span class="line"></span><br><span class="line">    // setup test topics</span><br><span class="line">    inputTopic = testDriver.createInputTopic(&quot;input-topic&quot;, stringSerde.serializer(), longSerde.serializer());</span><br><span class="line">    outputTopic = testDriver.createOutputTopic(&quot;result-topic&quot;, stringSerde.deserializer(), longSerde.deserializer());</span><br><span class="line"></span><br><span class="line">    // pre-populate store</span><br><span class="line">    store = testDriver.getKeyValueStore(&quot;aggStore&quot;);</span><br><span class="line">    store.put(&quot;a&quot;, 21 L);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@After</span><br><span class="line">public void tearDown() &#123;</span><br><span class="line">    testDriver.close();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Test</span><br><span class="line">public void shouldFlushStoreForFirstInput() &#123;</span><br><span class="line">    inputTopic.pipeInput(&quot;a&quot;, 1 L);</span><br><span class="line">    assertThat(outputTopic.readKeyValue(), equalTo(new KeyValue &lt; &gt; (&quot;a&quot;, 21 L)));</span><br><span class="line">    assertThat(outputTopic.isEmpty(), is(true));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Test</span><br><span class="line">public void shouldNotUpdateStoreForSmallerValue() &#123;</span><br><span class="line">    inputTopic.pipeInput(&quot;a&quot;, 1 L);</span><br><span class="line">    assertThat(store.get(&quot;a&quot;), equalTo(21 L));</span><br><span class="line">    assertThat(outputTopic.readKeyValue(), equalTo(new KeyValue &lt; &gt; (&quot;a&quot;, 21 L)));</span><br><span class="line">    assertThat(outputTopic.isEmpty(), is(true));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Test</span><br><span class="line">public void shouldNotUpdateStoreForLargerValue() &#123;</span><br><span class="line">    inputTopic.pipeInput(&quot;a&quot;, 42 L);</span><br><span class="line">    assertThat(store.get(&quot;a&quot;), equalTo(42 L));</span><br><span class="line">    assertThat(outputTopic.readKeyValue(), equalTo(new KeyValue &lt; &gt; (&quot;a&quot;, 42 L)));</span><br><span class="line">    assertThat(outputTopic.isEmpty(), is(true));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Test</span><br><span class="line">public void shouldUpdateStoreForNewKey() &#123;</span><br><span class="line">    inputTopic.pipeInput(&quot;b&quot;, 21 L);</span><br><span class="line">    assertThat(store.get(&quot;b&quot;), equalTo(21 L));</span><br><span class="line">    assertThat(outputTopic.readKeyValue(), equalTo(new KeyValue &lt; &gt; (&quot;a&quot;, 21 L)));</span><br><span class="line">    assertThat(outputTopic.readKeyValue(), equalTo(new KeyValue &lt; &gt; (&quot;b&quot;, 21 L)));</span><br><span class="line">    assertThat(outputTopic.isEmpty(), is(true));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Test</span><br><span class="line">public void shouldPunctuateIfEvenTimeAdvances() &#123;</span><br><span class="line">    final Instant recordTime = Instant.now();</span><br><span class="line">    inputTopic.pipeInput(&quot;a&quot;, 1 L, recordTime);</span><br><span class="line">    assertThat(outputTopic.readKeyValue(), equalTo(new KeyValue &lt; &gt; (&quot;a&quot;, 21 L)));</span><br><span class="line"></span><br><span class="line">    inputTopic.pipeInput(&quot;a&quot;, 1 L, recordTime);</span><br><span class="line">    assertThat(outputTopic.isEmpty(), is(true));</span><br><span class="line"></span><br><span class="line">    inputTopic.pipeInput(&quot;a&quot;, 1 L, recordTime.plusSeconds(10 L));</span><br><span class="line">    assertThat(outputTopic.readKeyValue(), equalTo(new KeyValue &lt; &gt; (&quot;a&quot;, 21 L)));</span><br><span class="line">    assertThat(outputTopic.isEmpty(), is(true));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">@Test</span><br><span class="line">public void shouldPunctuateIfWallClockTimeAdvances() &#123;</span><br><span class="line">    testDriver.advanceWallClockTime(Duration.ofSeconds(60));</span><br><span class="line">    assertThat(outputTopic.readKeyValue(), equalTo(new KeyValue &lt; &gt; (&quot;a&quot;, 21 L)));</span><br><span class="line">    assertThat(outputTopic.isEmpty(), is(true));</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static class CustomMaxAggregatorSupplier implements ProcessorSupplier &lt; String, Long &gt; &#123;</span><br><span class="line">    @Override</span><br><span class="line">    public Processor &lt; String,</span><br><span class="line">    Long &gt; get() &#123;</span><br><span class="line">        return new CustomMaxAggregator();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">public static class CustomMaxAggregator implements Processor &lt; String, Long &gt; &#123;</span><br><span class="line">    ProcessorContext context;</span><br><span class="line">    private KeyValueStore &lt; String,</span><br><span class="line">    Long &gt; store;</span><br><span class="line"></span><br><span class="line">    @SuppressWarnings(&quot;unchecked&quot;)</span><br><span class="line">    @Override</span><br><span class="line">    public void init(final ProcessorContext context) &#123;</span><br><span class="line">        this.context = context;</span><br><span class="line">        context.schedule(Duration.ofSeconds(60), PunctuationType.WALL_CLOCK_TIME, time - &gt; flushStore());</span><br><span class="line">        context.schedule(Duration.ofSeconds(10), PunctuationType.STREAM_TIME, time - &gt; flushStore());</span><br><span class="line">        store = (KeyValueStore &lt; String, Long &gt; ) context.getStateStore(&quot;aggStore&quot;);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void process(final String key, final Long value) &#123;</span><br><span class="line">        final Long oldValue = store.get(key);</span><br><span class="line">        if (oldValue == null || value &gt; oldValue) &#123;</span><br><span class="line">            store.put(key, value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    private void flushStore() &#123;</span><br><span class="line">        final KeyValueIterator &lt; String, Long &gt; it = store.all();</span><br><span class="line">        while (it.hasNext()) &#123;</span><br><span class="line">            final KeyValue &lt; String, Long &gt; next = it.next();</span><br><span class="line">            context.forward(next.key, next.value);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    @Override</span><br><span class="line">    public void close() &#123;&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>



<h3 id="结语及参考"><a href="#结语及参考" class="headerlink" title="结语及参考"></a>结语及参考</h3><p>本文简单介绍了下 kafka streams 的基本概念、实现架构和简单使用，kakfa streams 本身提供了功能完备的流计算能力，是一款深度结合kafka 的轻量级的流式计算引擎。</p>
<p>以下是一些参考资料</p>
<ul>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://kafka.apache.org/documentation/streams/">https://kafka.apache.org/documentation/streams/</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="https://flink.apache.org/usecases.html">https://flink.apache.org/usecases.html</a></li>
<li><a target="_blank" rel="external nofollow noopener noreferrer" href="http://www.jasongj.com/kafka/kafka_stream/">http://www.jasongj.com/kafka/kafka_stream/</a></li>
</ul>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/kafka-streams/" rel="tag"># kafka streams</a>
              <a href="/tags/%E6%B5%81%E8%AE%A1%E7%AE%97/" rel="tag"># 流计算</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/posts/413a7d5a/" rel="prev" title="helm 源码剖析">
      <i class="fa fa-chevron-left"></i> helm 源码剖析
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          
    
  <div class="comments">
    <div id="disqus_thread">
      <noscript>Please enable JavaScript to view the comments powered by Disqus.</noscript>
    </div>
  </div>
  

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka-streams-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-number">1.</span> <span class="nav-text">kafka streams 是什么</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B5%81%E8%AE%A1%E7%AE%97%E7%AE%80%E4%BB%8B"><span class="nav-number">1.1.</span> <span class="nav-text">流计算简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka-streams"><span class="nav-number">1.2.</span> <span class="nav-text">kafka streams</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#kafka-streams-%E5%90%8C%E5%85%B6%E4%BB%96%E6%B5%81%E8%AE%A1%E7%AE%97%E6%A1%86%E6%9E%B6%E5%AF%B9%E6%AF%94"><span class="nav-number">1.3.</span> <span class="nav-text">kafka streams 同其他流计算框架对比</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kakfa-streams-%E7%A4%BA%E4%BE%8B%EF%BC%9A-WordCount"><span class="nav-number">2.</span> <span class="nav-text">kakfa streams 示例： WordCount</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka-streams-%E7%9A%84%E6%9E%B6%E6%9E%84"><span class="nav-number">3.</span> <span class="nav-text">kafka streams 的架构</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%B9%B6%E5%8F%91%E6%A8%A1%E5%9E%8B"><span class="nav-number">3.1.</span> <span class="nav-text">并发模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BC%A9%E5%AE%B9"><span class="nav-number">3.1.1.</span> <span class="nav-text">缩容</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%89%A9%E5%AE%B9"><span class="nav-number">3.1.2.</span> <span class="nav-text">扩容</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%8A%A8%E6%80%81%E5%88%86%E9%85%8D-tasks"><span class="nav-number">3.1.3.</span> <span class="nav-text">动态分配 tasks</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E9%99%90%E5%88%B6"><span class="nav-number">3.1.4.</span> <span class="nav-text">限制</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9C%89%E5%90%91%E6%97%A0%E7%8E%AF%E5%9B%BE%EF%BC%88Topology%EF%BC%89"><span class="nav-number">3.2.</span> <span class="nav-text">有向无环图（Topology）</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%8A%B6%E6%80%81%E5%AD%98%E5%82%A8"><span class="nav-number">3.3.</span> <span class="nav-text">状态存储</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka-streams-%E4%B8%8E%E6%B5%81%E8%AE%A1%E7%AE%97"><span class="nav-number">4.</span> <span class="nav-text">kafka streams 与流计算</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%97%B6%E9%97%B4"><span class="nav-number">4.1.</span> <span class="nav-text">时间</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%AA%97%E5%8F%A3"><span class="nav-number">4.2.</span> <span class="nav-text">窗口</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%89%A7%E8%A1%8C%E4%BF%9D%E8%AF%81"><span class="nav-number">4.3.</span> <span class="nav-text">执行保证</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka-streams-%E7%BC%96%E7%A8%8B%E6%A8%A1%E5%9E%8B"><span class="nav-number">5.</span> <span class="nav-text">kafka streams 编程模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Processor-API"><span class="nav-number">5.1.</span> <span class="nav-text">Processor API</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DSL"><span class="nav-number">5.2.</span> <span class="nav-text">DSL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#DSL-%E5%92%8C-Processor-API-%E7%9A%84%E7%BB%93%E5%90%88"><span class="nav-number">5.3.</span> <span class="nav-text">DSL 和 Processor API 的结合</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E8%AF%AD%E5%8F%8A%E5%8F%82%E8%80%83"><span class="nav-number">5.4.</span> <span class="nav-text">结语及参考</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope="" itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">大飞哥</p>
  <div class="site-description" itemprop="description">大数据、云原生、微服务等</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives">
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">6</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
        <span class="site-state-item-count">28</span>
        <span class="site-state-item-name">tags</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/hustclf" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;hustclf" rel="external nofollow noopener noreferrer" target="_blank"><i class="github fa-fw"></i>GitHub</a>
      </span>
      <span class="links-of-author-item">
        <a href="mailto:hustclf@gmail.com" title="E-Mail → mailto:hustclf@gmail.com" rel="external nofollow noopener noreferrer" target="_blank"><i class="envelope fa-fw"></i>E-Mail</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://stackoverflow.com/users/8595519/lifei-chen" title="StackOverflow → https:&#x2F;&#x2F;stackoverflow.com&#x2F;users&#x2F;8595519&#x2F;lifei-chen" rel="external nofollow noopener noreferrer" target="_blank"><i class="stack-overflow fa-fw"></i>StackOverflow</a>
      </span>
      <span class="links-of-author-item">
        <a href="https://www.linkedin.com/in/lifei-chen-b53568122/" title="Linkedin → https:&#x2F;&#x2F;www.linkedin.com&#x2F;in&#x2F;lifei-chen-b53568122&#x2F;" rel="external nofollow noopener noreferrer" target="_blank"><i class="linkedin fa-fw"></i>Linkedin</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">大飞哥</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="external nofollow noopener noreferrer" target="_blank">NexT.Gemini</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  

<script>
  var disqus_config = function() {
    this.page.url = "https://hustclf.github.io/posts/cfee7ffb/";
    this.page.identifier = "posts/cfee7ffb/";
    this.page.title = "kafka streams：一款轻量级流式计算引擎";
    };
  NexT.utils.loadComments(document.querySelector('#disqus_thread'), () => {
    if (window.DISQUS) {
      DISQUS.reset({
        reload: true,
        config: disqus_config
      });
    } else {
      var d = document, s = d.createElement('script');
      s.src = 'https://hustclf.disqus.com/embed.js';
      s.setAttribute('data-timestamp', '' + +new Date());
      (d.head || d.body).appendChild(s);
    }
  });
</script>

</body>
</html>
